% vim: spell spelllang=en:
\input{preamble}

\usepackage[justification=centering]{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{lipsum}

\usepackage{siunitx}
\usepackage{hyphenat}

\usepackage{xcolor}

\definecolor{LightGray}{rgb}{0.83, 0.83, 0.83}
\definecolor{bg}{HTML}{282828}

\usepackage[newfloat]{minted}
\captionsetup[listing]{position=top}

\graphicspath{{figures/}}

\setminted{
style=monokai,
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
breaklines,
bgcolor=bg,
fontsize=\footnotesize,
linenos
}

\renewcommand\theadfont{\bfseries}

\title{
    PAR Laboratory Assignment\\
    Lab 5: Geometric (data) decomposition: \\
    heat diffusion equation
}

\author{
    par2109:
    Aleix Boné,
    Alex Herrero
}

\date{
    Spring 2019-20
}

\begin{document}

\thispagestyle{empty}
\clearpage
\setcounter{page}{-1}

\begin{titlepage}
{%
    \centering
    \null{}
    \vfill
    {\Huge \bfseries PAR Laboratory Assignment\par}
    \vspace{3em}
    {\Large {\scshape Lab 5:} 
    Geometric (data) decomposition: \\
    heat diffusion equation
\par}
    \vfill

    \includegraphics{images/jacobi}

    \vfill
    {\raggedleft{} \Large
        Aleix Boné\\
        Alex Herrero\\
        {\bfseries\ttfamily par2109}\\
        \vspace{4em}
        2020-06-05 % chktex 8
        \par}
}
\end{titlepage}

\tableofcontents
\pagebreak

\section{Introduction}%
\label{sec:introduction}

In this session we are going to work on the parallelization of a sequential code that
simulates heat diffusion in a solid body using two different solvers for the heat equation.
The program reads a configuration file that specifies the maximum number of steps, the size
of the body and the solver to use which can be either \emph{Jacobi} or \emph{Gauss-Seidel}.

The two solvers have different numerical properties and behave differently when parallelized.
With the \emph{Jacobi} method, the values of the \emph{i}th iteration remain unchanged until
the next iteration has been calculated, with \emph{Gauss-Seidel} the results are used immediately.
Moreover, \emph{Gauss-Seidel} converges faster than Jacobi.
\footnote{\url{https://www3.nd.edu/~zxu2/acms40390F12/Lec-7.3.pdf}}.



When we tried the sequential version of the program with the input in \texttt{test.dat}
we obtained different results as shown in figure~\ref{fig:mosaic_label} and 
\emph{Gauss-Seidel} was more than twice as fast than \emph{Jacobi}:

\begin{verbatim}
Iterations        : 25000
Resolution        : 254
Algorithm         : 1 (Gauss-Seidel)
Num. Heat sources : 2
   1: (0.00, 0.00) 1.00 2.50 
   2: (0.50, 1.00) 1.00 2.50 
Time: 2.394 
Flops and Flops per second: (8.806 GFlop => 3679.06 MFlop/s)
Convergence to residual=0.000050: 12409 iterations

Iterations        : 25000
Resolution        : 254
Algorithm         : 0 (Jacobi)
Num. Heat sources : 2
   1: (0.00, 0.00) 1.00 2.50 
   2: (0.50, 1.00) 1.00 2.50 
Time: 4.819 
Flops and Flops per second: (11.182 GFlop => 2320.50 MFlop/s)
Convergence to residual=0.000050: 15756 iterations
\end{verbatim}

% Sequential heat diffusion program
% differences between jacobi and gauss images

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{images/mosaic_label}
    \caption{Comparison between heat map generated by Jacobi and \emph{Gauss-Seidel}}%
    \label{fig:mosaic_label}
\end{figure}

\pagebreak
\section{Analysis of task granularities and dependences with \emph{Tareador}}%
\label{sec:analysis_of_task_granularities_and_dependences_with_tareador}

% dependency: sum on both jacobi and gauss

\begin{figure}[H]
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.2\linewidth]{dependency_coarse_jacobi}
    \caption{Coarse dependency graph of Jacobi}%
    \label{fig:dependency_coarse_jacobi}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
    \centering
    \includegraphics[width=0.2\linewidth]{dependency_coarse_gauss}
    \caption{Coarse dependency graph of \emph{Gauss-Seidel}}%
    \label{fig:dependency_coarse_gauss}
    \end{minipage}
\end{figure}

Figures~\ref{fig:dependency_coarse_jacobi} and \ref{fig:dependency_coarse_gauss}
show the dependency graph obtained with \emph{tareador} for Jacobi and \emph{Gauss-Seidel}
respectively (with very coarse granularity).

The first thing we notice is that there are two executions of
\texttt{relax\_jacobi} and \texttt{relax\_gauss}.  Taking a look at the code
shown in listing~\ref{lst:heat_while} we can see that the program applies the
relax function on the data until the residual obtained is small enough or the
maximum number of iterations is reached. Each iteration depends on the results
of the previous one, so they cannot be parallelized. In the case ran by
\emph{tareador} this is barely noticeable since the data used is very small and
the maximum number of iterations is set 2 with a resolution of 4. However for
\texttt{test.dat} the maximum number of iterations is set to 2500 with a resolution
of 254.

In the case of Jacobi there is also the function \texttt{copy\_mat} that is
used as an auxiliary matrix, since Jacobi does not modify \texttt{u} in place
and \emph{Gauss-Seidel} does.  In fact, this is the main difference between the two
algorithms, Jacobi uses the matrix from the previous iteration of the method
while \emph{Gauss-Seidel} uses modifies it as it traverses the matrix.

\begin{listing}[H]
\caption{heat.c}%
\label{lst:heat_while}
\inputminted[firstline=83,lastline=104]{c}{code/heat.c}
\end{listing}

In order to further analysis the potential parallelization of both both methods,
we have to look into the inner workings of each step of the relax functions. To
achieve this, we added finer \emph{tareador} task definitions, defining tasks
for each iteration of the loops as shown in Listing~\ref{lst:solver-tar}:

\begin{listing}[H]
\caption{solver-tareador.c}%
\label{lst:solver-tar}
\inputminted[firstline=28,lastline=43]{c}{code/solver-tareador.c}
\vspace{-2.5em}
\inputminted[firstline=56,lastline=72]{c}{code/solver-tareador.c}
\end{listing}

\begin{figure}[H]
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=6cm]{dependency_fine_jacobi.png}
        \caption{Fine dependency graph \\ Jacobi}%
        \label{fig:dependency_fine_jacobi}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=6cm]{dependency_fine_gauss.png}
        \caption{Fine dependency graph \\ \emph{Gauss-Seidel}}%
        \label{fig:dependency_fine_gauss}
    \end{minipage}
\end{figure}

Figure~\ref{fig:dependency_fine_jacobi} and~\ref{fig:dependency_fine_gauss} show the
task dependency graph obtained with \emph{tareador}. The results are disappointing, since
there are dependencies making all the code sequential, nevertheless, if we look at the
variables causing those dependencies (shown in figures~\ref{fig:dependence_jacobi}
and~\ref{fig:dependence_gauss}) we see that the dependence in both cases is caused by
the \texttt{sum} variable which in this case is only used once and can be reduced or
protected by an atomic clause. This means that we can still parallelize most of the
work.

However in the case of \emph{Gauss-Seidel} there is another dependency corresponding to some
memory positions in the heap, which by analyzing the code we can determine that this
data on the heap is the matrix \texttt{u}. Since \emph{Gauss-Seidel} modifies the data in
\texttt{u}, this creates a dependency between iterations of the method. More specifically
they have a dependency on the left cell and the top cell of the matrix.

\begin{figure}[H]
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{dependence_jacobi}
        \caption{Task dependence \\ Jacobi}%
        \label{fig:dependence_jacobi}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{dependence_gauss}
        \caption{Task dependence \\ \emph{Gauss-Seidel}}%
        \label{fig:dependence_gauss}
    \end{minipage}
\end{figure}

\begin{figure}[H]
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=6cm]{dependency_fine_jacobi_nosum}
        \caption{Fine dependency graph \\ Jacobi (no sum)}%
        \label{fig:dependency_fine_jacobi_nosum}
    \end{minipage}
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[height=6cm]{dependency_fine_gauss_nosum}
        \caption{Fine dependency graph \\ \emph{Gauss-Seidel} (no sum)}%
        \label{fig:dependency_fine_gauss_nosum}
    \end{minipage}
\end{figure}

In figure~\ref{fig:dependency_fine_jacobi_nosum} and~\ref{fig:dependency_fine_gauss_nosum}
we show the data decomposition obtained with \emph{tareador} ignoring the \texttt{sum} variable
dependency.
We can see that if we eliminate the sum dependency in Jacobi the iterations of the
loop inside \texttt{relax\_jacobi} are embarrassingly parallel. With \emph{Gauss-Seidel,}
we can clearly see the top-left dependency of the iterations caused by the modification
of the matrix \texttt{u}.


\pagebreak
\section{OpenMP parallelization and execution analysis: \emph{Jacobi}}

\begin{listing}[H]
    \caption{solver.c}%
    \label{lst:solver-omp}
    \inputminted[firstline=17,lastline=39]{c}{code/solver-omp-orig.c}
\end{listing}

If we examine \texttt{solver.c} (listing~\ref{lst:solver-omp} we see that there are some differences 
from the \emph{tareador} version,
mainly that there is an additional outer for loop that is repeated \texttt{howmany} times and determines the
range of the \texttt{i} variable of the first \texttt{for}. If we look at what \texttt{lowerb} and
\texttt{upperb} (listing~\ref{lst:heat_h}) we can see that they compute the index bounds of the block
number \texttt{id} if there are \texttt{n} indices divided in \texttt{p} blocks.

The code splits the matrix in \texttt{howmany} (in this case 4) blocks of rows and computes them.
We decided to use this division as a basis for our OpenMP implementation.

\begin{listing}[H]
    \caption{heat.h \texttt{\#define}}%
    \label{lst:heat_h}
    \inputminted[firstline=59,lastline=61]{c}{code/heat.h}
\end{listing}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{jacobi_blocks}
    \caption{Jacobi row block division with 4 threads on a 8x8 matrix}%
    \label{fig:blocks-jacobi}
\end{figure}

Figure~\ref{fig:blocks-jacobi} shows an example of the block division by rows performed on a 8x8 matrix
with 4 threads. In cases where the number of threads does not divide the dimension of the matrix, the final
block would be smaller to make up the difference.

As we discussed on the \emph{tareador} analysis, the only dependency between tasks is the variable
\texttt{sum}, so to guarantee that there are no data races we must apply a reduction. Given that we cannot use
the \texttt{pragma omp for} directives, we decided to create a variable \texttt{sum\_tmp} private on each
task to store the sum of each task and then perform an atomic addition to \texttt{sum} at the end of the tasks.
We also added \texttt{private(diff)} and \texttt{firstprivate(blockid)} since they where declared before the
task (blockid has to be firstprivate since the value is needed).
Listing~\ref{lst:jacobi_first} shows the first version of our code 

\begin{listing}[H]
    \caption{solver-omp.c Initial OpenMP version of jacobi method}%
    \label{lst:jacobi_first}
    \inputminted[firstline=26,lastline=55]{c}{code/solver-omp-jacobi-v1.c}
\end{listing}

Our initial version of the code did not modify the variable \texttt{howmany} which meant that we always
divided the matrix in 4 blocks which is not optimal since if we have more than 4 threads there are threads
that are not doing any work. This can be seen in the \emph{paraver} trace we generated and that we show in
figure~\ref{fig:trace-jacobi0} where we can see that there are always 4 concurrent tasks and 4 threads are unused.

We also noticed that there where huge sequential sections, which were caused by the matrix copy operation.
In our analysis with \emph{tareador} we saw that there were no dependencies between loop iterations so
we decided to parallelize it to remove this bottleneck. Our first approach, which was quite naive, was to
assign a task to every iteration of the loop, however the code run much slower than the sequential
version due to the massive overhead of the task creation, we decided to use the same geometric data
decomposition used in the \emph{Jacobi} function.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{jacobi_0_tasks}
    \caption{Parallel tasks Jacobi}%
    \label{fig:trace-jacobi0}
\end{figure}

Listing~\ref{lst:jacobi} shows the improved version of \emph{Jacobi} with improvements on \texttt{copy\_mat}
and \texttt{relax\_jacobi}. We removed the outer \texttt{for}
since is not needed as we use the parallel region with all its threads to divide the matrix. We also
applied the same division and parallelization approach to the \texttt{copy\_mat} function.
In figure~\ref{fig:trace-jacobi1} we can see the improvement over the previous version and how this time
there are much more tasks and all threads are used in parallel, although some have more work than others.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{jacobi_1_tasks}
    \caption{Parallel tasks jacobi improved}%
    \label{fig:trace-jacobi1}
\end{figure}

\begin{listing}[H]
    \caption{solver-omp.c Improved jacobi}%
    \label{lst:jacobi}
    \inputminted[firstline=8,lastline=19]{c}{code/solver-omp.c}
    \vspace{-2.5em}
    \inputminted[firstline=25,lastline=52]{c}{code/solver-omp.c}
\end{listing}

\pagebreak

In figure~\ref{fig:strong-jacobi} we can see the time and speed-up plots for different number 
of OMP threads of the improved version.
We can see that the execution time goes from ~5 seconds to less than 1 second
achieving a very good speed-up till around 9 threads, this is probably caused by the task creation
overhead.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{heat-omp-strong-jacobi-crop}
    \caption{Strong scalability graph \\ Jacobi}%
    \label{fig:strong-jacobi}
\end{figure}


\pagebreak
\section{OpenMP parallelization and execution analysis: \emph{Gauss-Seidel}}

% Describe how did you implement the parallelization strategy for the Gauss-Seidel solver and how did you guarantee the proper synchronization between threads.

A row or column based decomposition would not work in this case, since as we have seen in the
\emph{tareador} analysis, each iteration depends on the value of the previous row and column,
which means that if we implemented it like that the resulting code would be sequential.

The best approach is to divide the matrix in blocks and define the dependencies between the
iterations using. Figure~\ref{fig:blocks-gauss} shows an example
of the block decomposition and the dependencies in blue. For the task corresponding to
the block $i,j$ to begin executing, it must wait for both $i-1, j$ and $i, j-1$ to finish.

Listing~\ref{lst:gauss} shows the OpenMP implementation of the \emph{Gauss-Seidel} method. Notice that we create 2
outer for loops to divide the matrix both by rows and by columns. We define a reduction on the \texttt{sum}
variable that created one of the data dependencies and we define 2 sinks depending on
the block one row before and the other on the block on the column before.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{gauss_blocks}
    \caption{Gauss block decomposition with 4 threads on a 8x8 matrix}%
    \label{fig:blocks-gauss}
\end{figure}

\begin{listing}[H]
    \caption{solver-omp.c \emph{Gauss-Seidel}}%
    \label{lst:gauss}
    \inputminted[firstline=57,lastline=89]{c}{code/solver-omp.c}
\end{listing}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{heat-omp-strong-gauss-crop}
    \caption{Strong scalability graph \\ \emph{Gauss-Seidel}}%
    \label{fig:strong-gauss}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{thread_state_heat_omp_8}
    \caption{Thread State \emph{Gauss-Seidel}}%
    \label{fig:thread-gauss}
\end{figure}

As in the previous section, in figure~\ref{fig:strong-gauss} we can see the time and speed-up plots for different number 
of OMP threads, but in this case, of the execution of the \emph{Gauss-Seidel} solver. We can observe how the execution time improves with the number of threads, and the speed-up gets better. But, as we can see, the speed-up is not even close to the ideal speed-up line and performs far worse than the \emph{Jacobi} version.
This is caused due to the synchronization overhead introduced by the data dependencies, if we have more threads and
therefore we split the matrix in more blocks, we have to keep track and synchronize more tasks which generates an
overhead. Therefore, we should determine the block size that gives the best ratio between synchronization
and computation by choosing the proper block size.

% Finally explain how did you obtain the optimum value for the ratio computation/synchronization in the parallelization of this solver for 8 threads.

\pagebreak

The size of the block in our code depends on the value of \texttt{howmany}, that we decided to set to the
number of threads in the parallel region: if we have 8 threads we divide the matrix in $8\times 8$ blocks.
However by setting a different value we can change the number and therefor the size of the blocks used.
We tried changing the value of \texttt{howmany} form 2 to 32 
\footnote{We modified the program to accept howmany as a parameter}
and executing the program with input \texttt{"test.dat"} and 8 threads.
The best results where with \texttt{howmany = 16}. In figure~\ref{fig:howmany}
we can see a plot of the execution times based on the value of \texttt{howmany} where we can see that
the inflection point happens around \texttt{howmany = 16}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/results}
    \caption{Execution time vs. howmany (\emph{Gauss-Seidel})}%
    \label{fig:howmany}
\end{figure}


\section{Conclusions}%
\label{sec:conclusions}


Although the sequential version of \emph{Gauss-Seidel} greatly outperformed \emph{Jacobi} when
parallelizing the code as we have seen \emph{Jacobi} allows for a much greater parallelization
due to the lack dependencies between loop iterations. This shows that although an algorithm may
seem slow when compared to another in its sequential version, it may be more parallelizable than
the other.

\end{document}
